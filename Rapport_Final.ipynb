{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT3700 A2023 - TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cadre de notre travail final pour le cours IFT3700 A2023, notre équipe composée de [TODO: Noms des Membres] a entrepris la tâche de collecter, nettoyer et analyser des données numériques et catégoriques relatives à divers pays du monde. Ces données ont été extraites de 40 tableaux provenant de sources fiables sur Wikipédia (disponible dans l'énoncé du TP). Notre objectif était de nettoyer ces données, de les discrétiser en format binaire et de mener une analyse approfondie pour en extraire des insights pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que certains imports sont nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pour une meilleure visualisation de notre travail, retrouvez notre repository au lien suivant:\n",
    "https://github.com/mildshield14/ift3700_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthodologie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons collecté les données à partir des sources spécifiées dans l'énoncé. Nous avons utilisé des scripts afin de mener à bien le webscraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre script est disponible ci-dessous : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "from urllib.parse import unquote, urlsplit\n",
    "import re\n",
    "\n",
    "# Read URLs from the PHP file\n",
    "with open(\"view.php\", 'r') as file:\n",
    "    php_code = file.read()\n",
    "\n",
    "# Extract URLs using regex\n",
    "urls = re.findall(r'\"ulnk_url\":\"(https://en\\.wikipedia\\.org/wiki/[^\"]+)\"', php_code)\n",
    "\n",
    "# Initialize a counter for file names\n",
    "count = 1\n",
    "\n",
    "# Iterate through the URLs\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all tables with the specified class\n",
    "    tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "     # Get page title from the URL\n",
    "    page_title = unquote(urlsplit(url).path.split('/')[-1])\n",
    "\n",
    "    # Initialize an empty list to store DataFrames for each table on the current page\n",
    "    dfs_per_page = []\n",
    "\n",
    "    # Iterate through the list of tables and convert each to a DataFrame\n",
    "    for i, table in enumerate(tables):\n",
    "        df = pd.read_html(str(table), header=0)[0]  # Assuming headers are in the first row\n",
    "        dfs_per_page.append(df)\n",
    "\n",
    "\n",
    "        # Create a folder for each page if it doesn't exist in another folder (for duplicated columns)\n",
    "        page_folder = f'csv_duplicated_columns/{count}.{page_title}'\n",
    "        os.makedirs(page_folder, exist_ok=True)\n",
    "        \n",
    "        # Create a folder for each page if it doesn't exist\n",
    "        page_folder = f'csv_rawdata/{count}.{page_title}'\n",
    "        os.makedirs(page_folder, exist_ok=True)\n",
    "\n",
    "        # Write each DataFrame to a separate CSV file\n",
    "        filename = f'{page_folder}/{count}.{i + 1} table.csv'\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            df.to_csv(file, index=False)\n",
    "\n",
    "\n",
    "    # Increment the counter\n",
    "    count += 1\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons commencé par extraire les colonnes souhaitées pour chaque fichier à l'aide de scripts. Ainsi, nous nous sommes retrouvés avec ce que l'on appelle les \"raw data\".\n",
    "\n",
    "Cela a déclenché la phase complexe et longue du nettoyage des données.\n",
    "\n",
    "De ce fait, chacun des memebres de l'équipe avait pour but de nettoyer les données de 10 fichiers.\n",
    "\n",
    "Voici un exemple de sacript utilisé pour avoir un fichier à 2 colonnes; pays-carcteristique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv(\"/Users/vennilasooben/Downloads/ift3700_tp/csv_duplicated_columns/9.Importance_of_religion_by_country/9.1 cleaned.csv\")\n",
    "print(df.columns)\n",
    "# Select only the desired columns\n",
    "selected_columns = ['Country', 'Importance of Religion(9)']\n",
    "df = df[selected_columns]\n",
    "\n",
    "# Clean the \"Country\" column\n",
    "df['Country'] = df['Country'].str.replace(r'[^A-Za-z0-9\\s]+', '', regex=True)\n",
    "\n",
    "df['Importance of Religion(9)'] = df['Importance of Religion(9)'].str.replace('%', '')\n",
    "df['Importance of Religion(9)'] = pd.to_numeric(df['Importance of Religion(9)'], errors='coerce')\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv('9.1 cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons rencontré plusieurs problèmes que nous énumérerons, tout en proposant également les solutions que nous avons appliquées.\n",
    "\n",
    "##### 1) Noms différents dans chaque fichier\n",
    "\n",
    "Pour résoudre ce problème, notre première idée fut d'utiliser les concepts de distances et de similarités vus en cours afin de trouver les pays que nous souhaitons qui seraient prédéfinis. \n",
    "\n",
    "Cependant, entre DR, Democratic Republic of Congo, Congo, Republic of Congo, etc., et le conseil du professeur en classe de ne pas abuser des scripts, nous avons pris la décision de le faire manuellement. \n",
    "\n",
    "Ainsi, nous avons utiliser deux scripts qui permettront de mieux visualiser et faire uniquement des changements que l'on veut.\n",
    "\n",
    "- Le premier script permet d'obtenir une aide visuelle des pays en ordre alphabétique. Cela permet de voir des différences telles que \"United States\" et \"United States of America\".\n",
    "\n",
    "- Ensuite, nous avons utilisé un deuxième script qui parcourt le dossier de fichiers CSV et qui cherche la chaîne de caractères à remplacer et la remplace. En effet nous mettons en entrée \"United States\" et \"United States of America\" et le remplacement se fait de manière automatique.\n",
    "\n",
    "Vous trouverez ci-dessous les deux scripts utilisés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script permettant d'obtenir une aide visuelle des noms de chaque pays.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/vennilasooben/Downloads/ift3700_tp/cleaning_whole_data/final_table.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the first column with names\n",
    "names_column = df.iloc[:, 0]\n",
    "\n",
    "# Create a dictionary to store data based on the first letter\n",
    "data_by_first_letter = {}\n",
    "for name in names_column:\n",
    "    first_letter = name[0].upper()\n",
    "    if first_letter not in data_by_first_letter:\n",
    "        data_by_first_letter[first_letter] = []\n",
    "    data_by_first_letter[first_letter].append(name)\n",
    "\n",
    "# Create a new DataFrame based on the dictionary\n",
    "new_df = pd.DataFrame(data_by_first_letter.items(), columns=['First Letter', 'Names'])\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "new_csv_file_path = 'organized_data_country_names.csv'\n",
    "new_df.to_csv(new_csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been organized and saved to {new_csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script permettant de remplacer les noms des pays\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_folder(folder_path, replace_this, replace_with):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                if \"Country\" in df.columns:\n",
    "                    # Replace the specified string in the \"Country\" column\n",
    "                    df[\"Country\"] = df[\"Country\"].str.replace(replace_this, replace_with)\n",
    "\n",
    "                    # Save the modified DataFrame back to the same file\n",
    "                    df.to_csv(file_path, index=False)\n",
    "\n",
    "def main():\n",
    "    folder_path = \"/Users/vennilasooben/Downloads/ift3700_tp/csv_duplicated_columns\"\n",
    "    replace_this = input(\"replacw what?\")\n",
    "    replace_with = input(\"replace with what?\")\n",
    "    \n",
    "    process_folder(folder_path, replace_this, replace_with)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici des exemples de quelques conventions (dont les changements revenaient souvent) que nous avons trouvés pertinents afin de faciliter tous nos parsings.\n",
    "\n",
    "- United States\n",
    "- Democratic Republic of the Congo\n",
    "- Republic of the Congo\n",
    "- Myanmar\n",
    "- Macau\n",
    "- Ivory Coast\n",
    "- Guinea-Bissau\n",
    "- Sao Tome and Principe\n",
    "- British Virgin Islands\n",
    "- US Virgin Islands (par défaut si non mentionné; décision suite à une vérification de drapeau)\n",
    "- suppression de tous les accents\n",
    "- et plus encore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Danc certains cas, nous nous sommes retrouvés avec des valeurs en double voire triple.\n",
    "\n",
    "   - Ce à quoi on s'attendait:\n",
    "   \n",
    "   United Kingdom, England and Wales ainsi que United Kingdom, Great Britain and Northern Islands y étaient. Dans ce cas, nous avons pris la décision de faire la moyenne car la différence entre les 2 était de 0.2. Ainsi nous avons choisi 1.1.\n",
    "\n",
    "\n",
    "   - L'inattendu:\n",
    "   \n",
    "   Cependant, nous avons également fait face à des surprises dû à un manque de connaissance sur le sujet. Dans ce cas, des recherches ont été nécéssaires. \n",
    "   \n",
    "   Par exemple, Iraq et Iraq Kurdistan ne sont pas les mêmes. Bien que nous ayons supprimés Iraq Kurdistan dû à un manque important de données non-négligeables, pour cette étape du nettoyage, cette information a son importance car la même chose aurait pu arriver avec des pays différents si nos connaissances en géographie était vraiment limité..\n",
    "\n",
    "    (TODO: check if removed iraq kurdistan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de passer à l'étape gestion des valeurs manquantes, nous avons combiné les données. Ceci permet non seulemnt une meilleure visualisation mais aussi facilite la gestion car certains fichiers ont des pays que dautres nont pas, necessitanr ainsi eux aussi des donnees de remplacement.\n",
    "\n",
    "Voici de script qui merge le tout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_folder(folder_path, output_file):\n",
    "    all_data = pd.DataFrame(columns=[\"Country\"])\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if \"cleaned.csv\" in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                if \"Country\" in df.columns:\n",
    "                    # Extract the \"Country\" and the second column data\n",
    "                    country_column = df[\"Country\"]\n",
    "                    second_column_label = df.columns[1]  \n",
    "                    second_column_data = df.iloc[:, 1]\n",
    "\n",
    "                    # Create a DataFrame with \"Country\" and the second column data\n",
    "                    result_df = pd.DataFrame({\"Country\": country_column, f\"{second_column_label}\": second_column_data})\n",
    "\n",
    "                    # Merge with the existing data using \"Country\" as the key\n",
    "                    all_data = pd.merge(all_data, result_df, on=\"Country\", how=\"outer\")\n",
    "\n",
    "    all_data.to_csv(output_file, index=False)\n",
    "\n",
    "# Function to extract the number from the column names\n",
    "def extract_number_from_column(column_name):\n",
    "        try:\n",
    "            #asked chatgpt about regex to extract (number) from second columns name and got that answer\n",
    "          numbers = re.findall(r'\\((\\d+)\\)', column_name)\n",
    "          return tuple(map(int, numbers))\n",
    "        except ValueError:\n",
    "         print(\"put (number) in youe column header for example: Intentional Homicide Rate(4)\")\n",
    "         return float('inf')   # If no number is found, place it at the end for now temporarily\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder_path = \"/Users/vennilasooben/Downloads/ift3700_tp/csv_duplicated_columns\"\n",
    "    output_file = \"/Users/vennilasooben/Downloads/ift3700_tp/cleaning_whole_data/final_table.csv\"\n",
    "    process_folder(folder_path, output_file)\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(output_file)\n",
    "\n",
    "    # Sort the columns based on the numbers in parentheses\n",
    "    sorted_columns =  sorted(df.columns[1:], key=extract_number_from_column)\n",
    "\n",
    "    # Reorganize the DataFrame columns\n",
    "    df = df[[\"Country\"] + sorted_columns]\n",
    "\n",
    "    # Save the resulting DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrétisation des Caractéristiques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des Corrélations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul coeff de corr etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref lin etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation et Représentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO : Inclure mean median etc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrélations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO : Regr Lin, class bayes etc ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO- Inclure images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce trvail, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
